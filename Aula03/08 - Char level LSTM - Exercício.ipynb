{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551800a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "030132ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T21:01:16.130375Z",
     "start_time": "2021-06-08T21:01:04.316747Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaf698ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T21:02:47.586137Z",
     "start_time": "2021-06-08T21:02:47.523728Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "586c163e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T21:03:27.320300Z",
     "start_time": "2021-06-08T21:03:26.827614Z"
    }
   },
   "outputs": [],
   "source": [
    "#char encoding\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# text encoding\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8117b4d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T21:04:58.668538Z",
     "start_time": "2021-06-08T21:04:58.373052Z"
    }
   },
   "source": [
    "### Pre-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffa37ac4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T21:04:33.790945Z",
     "start_time": "2021-06-08T21:04:33.733073Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Inicializa array\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Preenche com valor 1\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Reshape\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5949a639",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T21:06:04.783446Z",
     "start_time": "2021-06-08T21:06:04.779481Z"
    }
   },
   "outputs": [],
   "source": [
    "# testar one_hot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80d30e23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T21:07:13.117748Z",
     "start_time": "2021-06-08T21:07:13.101402Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: DadosArray you want to make batches from\n",
    "       batch_size: Tamanho do batch\n",
    "       seq_length: Numero de caracteres encodados em cada sequencia\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3804f37",
   "metadata": {},
   "source": [
    "### Define a arquitetura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea4a97f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T21:10:42.346791Z",
     "start_time": "2021-06-08T21:10:42.316299Z"
    }
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        #definir lstm\n",
    "        \n",
    "        #definir dropout\n",
    "        \n",
    "        #definir camada fc \n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "               \n",
    "        # camada lstm\n",
    "        r_output, hidden = # \n",
    "        \n",
    "        # dropout\n",
    "        out = # \n",
    "        \n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        # camada fc\n",
    "        out = #\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Gera tensores de tamanho n_layers x betch_size x n_hidden\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aac4863a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T21:14:55.516052Z",
     "start_time": "2021-06-08T21:14:55.489016Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    #dados de treino/validacao\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encoding\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            # Cria vari√°veis para hidden state \n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # saida do modelo\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    \n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() \n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f262a84",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4d45111",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T21:14:06.220842Z",
     "start_time": "2021-06-08T21:14:06.144343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 64, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=64, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_hidden=64\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73833f9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T21:27:17.189955Z",
     "start_time": "2021-06-08T21:14:57.863247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5... Step: 10... Loss: 4.3304... Val Loss: 4.3099\n",
      "Epoch: 1/5... Step: 20... Loss: 3.7938... Val Loss: 3.6659\n",
      "Epoch: 1/5... Step: 30... Loss: 3.3357... Val Loss: 3.2176\n",
      "Epoch: 1/5... Step: 40... Loss: 3.2312... Val Loss: 3.1474\n",
      "Epoch: 1/5... Step: 50... Loss: 3.2396... Val Loss: 3.1303\n",
      "Epoch: 1/5... Step: 60... Loss: 3.1969... Val Loss: 3.1266\n",
      "Epoch: 1/5... Step: 70... Loss: 3.1740... Val Loss: 3.1248\n",
      "Epoch: 1/5... Step: 80... Loss: 3.1828... Val Loss: 3.1229\n",
      "Epoch: 1/5... Step: 90... Loss: 3.1814... Val Loss: 3.1222\n",
      "Epoch: 1/5... Step: 100... Loss: 3.1681... Val Loss: 3.1211\n",
      "Epoch: 1/5... Step: 110... Loss: 3.1701... Val Loss: 3.1203\n",
      "Epoch: 1/5... Step: 120... Loss: 3.1486... Val Loss: 3.1195\n",
      "Epoch: 1/5... Step: 130... Loss: 3.1591... Val Loss: 3.1184\n",
      "Epoch: 2/5... Step: 140... Loss: 3.1566... Val Loss: 3.1178\n",
      "Epoch: 2/5... Step: 150... Loss: 3.1592... Val Loss: 3.1172\n",
      "Epoch: 2/5... Step: 160... Loss: 3.1404... Val Loss: 3.1165\n",
      "Epoch: 2/5... Step: 170... Loss: 3.1161... Val Loss: 3.1156\n",
      "Epoch: 2/5... Step: 180... Loss: 3.1243... Val Loss: 3.1149\n",
      "Epoch: 2/5... Step: 190... Loss: 3.1245... Val Loss: 3.1139\n",
      "Epoch: 2/5... Step: 200... Loss: 3.1244... Val Loss: 3.1119\n",
      "Epoch: 2/5... Step: 210... Loss: 3.1241... Val Loss: 3.1095\n",
      "Epoch: 2/5... Step: 220... Loss: 3.1285... Val Loss: 3.1052\n",
      "Epoch: 2/5... Step: 230... Loss: 3.1117... Val Loss: 3.0993\n",
      "Epoch: 2/5... Step: 240... Loss: 3.1012... Val Loss: 3.0916\n",
      "Epoch: 2/5... Step: 250... Loss: 3.0955... Val Loss: 3.0817\n",
      "Epoch: 2/5... Step: 260... Loss: 3.0814... Val Loss: 3.0692\n",
      "Epoch: 2/5... Step: 270... Loss: 3.0660... Val Loss: 3.0512\n",
      "Epoch: 3/5... Step: 280... Loss: 3.0653... Val Loss: 3.0271\n",
      "Epoch: 3/5... Step: 290... Loss: 3.0266... Val Loss: 2.9990\n",
      "Epoch: 3/5... Step: 300... Loss: 3.0202... Val Loss: 2.9715\n",
      "Epoch: 3/5... Step: 310... Loss: 2.9897... Val Loss: 2.9434\n",
      "Epoch: 3/5... Step: 320... Loss: 2.9664... Val Loss: 2.9194\n",
      "Epoch: 3/5... Step: 330... Loss: 2.9380... Val Loss: 2.8993\n",
      "Epoch: 3/5... Step: 340... Loss: 2.9215... Val Loss: 2.8820\n",
      "Epoch: 3/5... Step: 350... Loss: 2.9213... Val Loss: 2.8663\n",
      "Epoch: 3/5... Step: 360... Loss: 2.8895... Val Loss: 2.8521\n",
      "Epoch: 3/5... Step: 370... Loss: 2.9030... Val Loss: 2.8374\n",
      "Epoch: 3/5... Step: 380... Loss: 2.8889... Val Loss: 2.8226\n",
      "Epoch: 3/5... Step: 390... Loss: 2.8670... Val Loss: 2.8109\n",
      "Epoch: 3/5... Step: 400... Loss: 2.8466... Val Loss: 2.7932\n",
      "Epoch: 3/5... Step: 410... Loss: 2.8371... Val Loss: 2.7765\n",
      "Epoch: 4/5... Step: 420... Loss: 2.8152... Val Loss: 2.7599\n",
      "Epoch: 4/5... Step: 430... Loss: 2.8028... Val Loss: 2.7412\n",
      "Epoch: 4/5... Step: 440... Loss: 2.7844... Val Loss: 2.7230\n",
      "Epoch: 4/5... Step: 450... Loss: 2.7482... Val Loss: 2.7055\n",
      "Epoch: 4/5... Step: 460... Loss: 2.7475... Val Loss: 2.6876\n",
      "Epoch: 4/5... Step: 470... Loss: 2.7380... Val Loss: 2.6701\n",
      "Epoch: 4/5... Step: 480... Loss: 2.7277... Val Loss: 2.6530\n",
      "Epoch: 4/5... Step: 490... Loss: 2.7137... Val Loss: 2.6376\n",
      "Epoch: 4/5... Step: 500... Loss: 2.7011... Val Loss: 2.6241\n",
      "Epoch: 4/5... Step: 510... Loss: 2.7110... Val Loss: 2.6115\n",
      "Epoch: 4/5... Step: 520... Loss: 2.7065... Val Loss: 2.5996\n",
      "Epoch: 4/5... Step: 530... Loss: 2.6604... Val Loss: 2.5888\n",
      "Epoch: 4/5... Step: 540... Loss: 2.6490... Val Loss: 2.5786\n",
      "Epoch: 4/5... Step: 550... Loss: 2.6570... Val Loss: 2.5735\n",
      "Epoch: 5/5... Step: 560... Loss: 2.6233... Val Loss: 2.5624\n",
      "Epoch: 5/5... Step: 570... Loss: 2.6355... Val Loss: 2.5538\n",
      "Epoch: 5/5... Step: 580... Loss: 2.6333... Val Loss: 2.5471\n",
      "Epoch: 5/5... Step: 590... Loss: 2.6210... Val Loss: 2.5392\n",
      "Epoch: 5/5... Step: 600... Loss: 2.5934... Val Loss: 2.5332\n",
      "Epoch: 5/5... Step: 610... Loss: 2.6166... Val Loss: 2.5268\n",
      "Epoch: 5/5... Step: 620... Loss: 2.5897... Val Loss: 2.5217\n",
      "Epoch: 5/5... Step: 630... Loss: 2.6313... Val Loss: 2.5147\n",
      "Epoch: 5/5... Step: 640... Loss: 2.6043... Val Loss: 2.5101\n",
      "Epoch: 5/5... Step: 650... Loss: 2.5933... Val Loss: 2.5047\n",
      "Epoch: 5/5... Step: 660... Loss: 2.5769... Val Loss: 2.4987\n",
      "Epoch: 5/5... Step: 670... Loss: 2.6020... Val Loss: 2.4950\n",
      "Epoch: 5/5... Step: 680... Loss: 2.5921... Val Loss: 2.4888\n",
      "Epoch: 5/5... Step: 690... Loss: 2.5618... Val Loss: 2.4846\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 5\n",
    "\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d50fc4",
   "metadata": {},
   "source": [
    "### Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c52e6bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T21:30:32.718908Z",
     "start_time": "2021-06-08T21:30:32.675145Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        \n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        \n",
    "        h = tuple([each.data for each in h])\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ffc86599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T21:30:39.453610Z",
     "start_time": "2021-06-08T21:30:39.431248Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    net.cpu()\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74e12943",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T21:30:40.824527Z",
     "start_time": "2021-06-08T21:30:40.303807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annas wilt an se to ne an to sos to hhas. \"hire wil hhit ose\n",
      "nhang an tot as the toed wot ate\n",
      "tis she as the win nesis nit on sot to sithed the, tal het tois\n",
      "tang thas antetang ho terans tor\n",
      "wet atee no nen he wolo tint, af sit on whe as shar hir whos. \"he tare se sererinte his hh afithe wot seed,\n",
      "we sor and so tas setos al sire tor wand af hos,,\n",
      "he til tee her singas\n",
      "tor terrand at hor,\n",
      "af nat sin he teisd sans sin tore af hor sot tare he and she sere tos tito al setile tan san sis ate sas werer. \"n\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 500, prime='Anna', top_k=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
